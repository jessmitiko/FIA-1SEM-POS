# FIA-1SEM-POS
Projeto da P√≥s-Gradua√ß√£o Big Data e Eng. de Dados da FIA.

# üìå Objetivo do Projeto
Constru√ß√£o de um pipeline de dados near realtime com atualiza√ß√£o de 2 em 2 minutos contento as posi√ß√µes das linhas de √¥nibus da cidade de S√£o Paulo, o projeto utiliza a API Olho Vivo disponibilizada gratuitamente pela Prefeitura de S√£o Paulo.  
\+ Como entreg√°vel adicional construimos um dashboard simples no Power BI para a apresenta√ß√£o dos dados j√° devidamente tratados.

## üõ†Ô∏è Arquitetura Inicial da Solu√ß√£o
![](/docs/Screenshot_2025-01-03_180617.png)  
 
## üõ†Ô∏è Arquitetura Final da Solu√ß√£o  
![](/docs/Screenshot_2025-01-03_182843.png)  
### Mudan√ßas do MVP para a arquitetura final
- O uso do delta foi descontinuado por conta de uma incompatibilidade entre o delta-core [maven lib] e a vers√£o do Spark utilizada na imagem Docker que est√°vamos utilizando, por conta disso substitu√≠mos os formatos das camadas `silver` e `gold` de delta para CSV.
- Descontinuamos o uso do NiFi, √© uma ferramenta muito limitada a configura√ß√£o da extra√ß√£o e transforma√ß√£o de dados via interface, o que dificulta a configura√ß√£o sistem√°tica via Docker Compose, nesse caso utilizamos o pr√≥prio cluster do Spark para executar a extra√ß√£o do dado da API.
- O Hive n√£o foi utilizado pela falta de tempo para estudar como configurar a ferramenta, o uso dele se fazia necess√°rio j√° que o Power BI precisa de uma SQL engine para acessar de forma mais simples o dado do MinIO, at√© onde entendi o Hive faz uso do Presto como SQL Engine. Como substituto utilizamos um script em Python dentro de uma Connection criada dentro do pr√≥prio arquivo .pbix.

# üñ•Ô∏è Como rodar o projeto localmente?
‚ò†Ô∏è _Testado somente no Windows, talvez em Linux d√™ algum pau por causa de estrutura de diret√≥rio e quebra de linha de arquivo, vai saber._ ‚ò†Ô∏è

1. Confirme que voc√™ tem o Docker instalado executando `docker --version` no terminal. E veja se o daemon do docker est√° rodando `docker container ls`.
2. ‚ö†Ô∏è [Crie uma chave de acesso no Site da SPTrans para usar a API Olho Vivo](https://www.sptrans.com.br/desenvolvedores) e use esta chave em todos os jobs de ingest√£o. ‚ö†Ô∏è
    - airflow/jobs/bronze/
        - ingestion-linhas.py
        - ingestion-paradas_by_linhas.py
        - ingestion-posicao_by_linha.py
3. Execute no terminal dentro do diret√≥rio raiz do projeto `docker compose up -d`.
4. Acesse o Airflow com o usu√°rio `airflow` e senha `airflow` [padr√£o na imagem Docker Oficial] e despause as DAGs com o prefixo `ingestion-` e `final-`.
5. [Adicional] Necess√°rio ter o Power BI Desktop instalado para abrir o dashboard.
    - dash/dashboard.pbix

## Acesso aos Containers Docker
- http://localhost:9001/ [MinIO]
- http://localhost:8889/ [Jupyter Notebook]
- http://localhost:8080/ [Airflow]
- http://localhost:38080/ [Spark]

# üõ†Ô∏è How-to
- Para criar novos jobs ou DAGs utilize os templates em:
    - airflow/dags/spark-submit-template.py
    - airflow/jobs/spark-job-template.py

# üìñ Links de Referencia
- [API DO OLHO VIVO - Guia de Refer√™ncia](https://www.sptrans.com.br/desenvolvedores/api-do-olho-vivo-guia-de-referencia/documentacao-api/)
- [Arquitetura medallion](https://www.databricks.com/br/glossary/medallion-architecture)
- [O que √© Delta Lake?](https://docs.databricks.com/pt/delta/index.html)
- [Presto SQL Engine](https://prestodb.io/)
